---
title: Links to Slides, Relevant Chapters in [WM22] and Extra Reading, and Presenters
year: 2023
---

Lecture 1
: [Introduction to Low-Dimensional Models]({{ site.baseurl }}/assets/slides/Lec1.pdf){:target="_blank"} {::comment}__{:/comment}
  : John Wright
: - Chapters 2-6, [WM22](https://book-wright-ma.github.io/){:target="_blank"} {::comment}__{:/comment}

Lecture 2
: [Learning Unrolled Networks for Sparse Recovery]({{ site.baseurl }}/assets/slides/Lec2.pdf){:target="_blank"} {::comment}__{:/comment}
  : Atlas Wang
: - J. Liu, X. Chen, Z. Wang, W. Yin, and H. Cai. “Towards Constituting Mathematical Structures for Learning to Optimize.” ICML 2023
- (α-β) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, and Z. Wang, W. Yin, “Learning to Optimize: A Primer and A Benchmark”, Journal of Machine Learning Research (JMLR), 2022
- X. Chen, J. Liu, Z. Wang, and W. Yin. “Hyperparameter Tuning is All You Need for LISTA.” NeurIPS 2021
- J. Liu, X. Chen, Z. Wang, and W. Yin. “ALISTA: Analytic weights are as good as learned weights in LISTA.” ICLR 2019
- X. Chen, J. Liu, Z. Wang, and W. Yin. “Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds.” NeurIPS 2018.

Lecture 3
: Design Deep Networks for Pursuing Low-Dimensional Structures<!--: [Design Deep Networks for Pursuing Low-Dimensional Structures]({{ site.baseurl }}/assets/slides/Lec5.pdf){:target="_blank"} {::comment}__{:/comment}-->
  : Yi Ma
: - Chapter 16, [WM22](https://book-wright-ma.github.io/){:target="_blank"} {::comment}__{:/comment}
- [ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction](https://jmlr.org/papers/v23/21-0631.html){:target="_blank"} {::comment}__{:/comment}
- [CTRL: Closed-Loop Transcription to an LDR via Minimaxing Rate Reduction](https://www.mdpi.com/1099-4300/24/4/456/htm){:target="_blank"} {::comment}__{:/comment}

Lecture 4
: Learning Low-Dimensional Models via Nonconvex Optimization<!--: [Learning Low-Dimensional Models via Nonconvex Optimization]({{ site.baseurl }}/assets/slides/Lec3.pdf){:target="_blank"} {::comment}__{:/comment}-->
  : Yuqian Zhang
: - Chapters 7, 9, [WM22](https://book-wright-ma.github.io/){:target="_blank"} {::comment}__{:/comment}
- Survey: ["From Symmetry to Geometry: Tractable Nonconvex Problems"](https://arxiv.org/abs/2007.06753){:target="_blank"} {::comment}__{:/comment}
  
Lectures 5--6
: Low-Dimensional Representations in Deep Networks<!--: [Low-Dimensional Representations in Deep Networks]({{ site.baseurl }}/assets/slides/Lec4.pdf){:target="_blank"} {::comment}__{:/comment}-->
  : Qing Qu, Zhihui Zhu
: - [A Geometric Analysis of Neural Collapse with Unconstrained Features](https://arxiv.org/abs/2105.02375){:target="_blank"} {::comment}__{:/comment}
- [On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features](https://arxiv.org/abs/2203.01238){:target="_blank"} {::comment}__{:/comment}
- [Robust Training under Label Noise by Over-parameterization](https://arxiv.org/abs/2202.14026){:target="_blank"} {::comment}__{:/comment}
- [The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks](https://arxiv.org/abs/2306.01154){:target="_blank"} {::comment}__{:/comment}
- [Principled and Efficient Transfer Learning of Deep Models via Neural Collapse ](https://arxiv.org/abs/2212.12206){:target="_blank"} {::comment}__{:/comment}

Lecture 7
: Deep Representation Learning from the Ground Up<!--: [Deep Representation Learning from the Ground Up]({{ site.baseurl }}/assets/slides/Lec4.pdf){:target="_blank"} {::comment}__{:/comment}-->
  : Sam Buchanan
: - Chapter 16, [WM22](https://book-wright-ma.github.io/){:target="_blank"} {::comment}__{:/comment}
- [Deep Networks and the Multiple Manifold Problem](https://openreview.net/forum?id=O-6Pm_d_Q-){:target="_blank"} {::comment}__{:/comment}
- [Deep Networks Provably Classify Data on Curves](https://arxiv.org/abs/2107.14324){:target="_blank"} {::comment}__{:/comment}
- [White-Box Transformers via Sparse Rate Reduction](https://arxiv.org/abs/2306.01129)
